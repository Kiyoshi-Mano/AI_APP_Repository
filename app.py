# KUBOTADiag.py

import os
import io
import re
import json
import datetime as dt
import unicodedata
from typing import List, Optional, Dict, Any
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from openai import OpenAI
from dateutil.relativedelta import relativedelta
from rapidfuzz import fuzz, process

# ---------- Config ----------
APP_TITLE = "kubotaå»ºæ©Ÿã€€ä¿®ç†æƒ…å ±ã‚¢ãƒ—ãƒª"
DATA_DIR = "KUBOTA_DiagDATA"
DEFAULT_FILES = [
    f"{DATA_DIR}/kubota_diagDATA_1_6.xlsm",
    f"{DATA_DIR}/kubota_diagDATA_7_12.xlsm",
]

# ---------- Utility Functions ----------

@st.cache_data(show_spinner=False)
def load_excel(paths: List[str], sheets: Optional[List[str]] = None) -> pd.DataFrame:
    """Load Excel files and concatenate sheets"""
    all_dataframes = []

    for path in paths:
        if not os.path.exists(path):
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
            continue

        try:
            excel_file = pd.ExcelFile(path, engine='openpyxl')
            available_sheets = excel_file.sheet_names

            if sheets is None:
                sheets_to_read = available_sheets
            else:
                sheets_to_read = [s for s in sheets if s in available_sheets]

            for sheet in sheets_to_read:
                # Try different header positions to find the correct one
                df = None
                for header_row in [0, 1]:
                    try:
                        temp_df = pd.read_excel(path, sheet_name=sheet, engine='openpyxl', header=header_row)
                        # Check if we have meaningful column names (not all "Unnamed")
                        unnamed_count = sum(1 for col in temp_df.columns if str(col).startswith('Unnamed'))
                        if unnamed_count < len(temp_df.columns) * 0.8:  # Less than 80% unnamed columns
                            df = temp_df
                            break
                    except:
                        continue

                if df is None:
                    # Fallback: try header=0
                    df = pd.read_excel(path, sheet_name=sheet, engine='openpyxl', header=0)

                # Add metadata columns
                df['ãƒ•ã‚¡ã‚¤ãƒ«å'] = os.path.basename(path)
                df['ã‚·ãƒ¼ãƒˆå'] = sheet
                all_dataframes.append(df)

        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {path}: {str(e)}")
            continue

    if not all_dataframes:
        return pd.DataFrame()

    # Concatenate all dataframes
    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)

    # Clean column names
    combined_df.columns = combined_df.columns.astype(str).str.strip()

    # Handle duplicate column names
    cols = list(combined_df.columns)
    seen = {}
    for i, col in enumerate(cols):
        if col in seen:
            seen[col] += 1
            cols[i] = f"{col}_{seen[col]}"
        else:
            seen[col] = 0
    combined_df.columns = cols

    # Replace "Â« NULL Â»" with NaN
    combined_df = combined_df.replace(["Â« NULL Â»", "", " ", "  "], np.nan)

    return combined_df

def normalize_text(text):
    """Normalize text: NFKC, full-width to half-width, trim spaces"""
    if pd.isna(text):
        return text
    text = str(text)
    text = unicodedata.normalize('NFKC', text)
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text

def parse_duration_to_days(duration_str):
    """Parse duration string like '27ã‚«æœˆ' or '15æ—¥' to days"""
    if pd.isna(duration_str):
        return np.nan

    duration_str = str(duration_str).strip()

    # Extract months
    month_match = re.search(r'(\d+)ã‚«æœˆ', duration_str)
    if month_match:
        return int(month_match.group(1)) * 30

    # Extract days
    day_match = re.search(r'(\d+)æ—¥', duration_str)
    if day_match:
        return int(day_match.group(1))

    # Try to extract just numbers
    number_match = re.search(r'(\d+)', duration_str)
    if number_match:
        return int(number_match.group(1))

    return np.nan

def create_hour_bins(hours):
    """Create hour meter bins"""
    if pd.isna(hours):
        return "ä¸æ˜"

    hours = float(hours)
    if hours < 250:
        return "0-250"
    elif hours < 500:
        return "250-500"
    elif hours < 1000:
        return "500-1000"
    elif hours < 2000:
        return "1000-2000"
    else:
        return "2000+"

def extract_bulletin_number(text):
    """Extract bulletin number from text"""
    if pd.isna(text):
        return np.nan

    text = str(text)
    # Look for patterns like K-XX-XXX
    bulletin_match = re.search(r'K-\d+-\d+', text)
    if bulletin_match:
        return bulletin_match.group(0)

    return np.nan

def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize and clean the dataframe"""
    if df.empty:
        return df

    df = df.copy()

    # Normalize text columns
    text_columns = df.select_dtypes(include=['object']).columns
    for col in text_columns:
        df[col] = df[col].apply(normalize_text)

    # Parse date columns
    date_columns = ['ä½œæ¥­å—ä»˜æ—¥', 'ä½œæ¥­é–‹å§‹æ—¥', 'ä½œæ¥­çµ‚äº†æ—¥', 'å£²ä¸Šæ—¥', 'ä¿è¨¼é–‹å§‹æ—¥', 'éƒ¨å“äº¤æ›æ—¥']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')

    # Parse numeric columns
    if 'ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿_HR' in df.columns:
        df['ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿_HR'] = pd.to_numeric(df['ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿_HR'], errors='coerce')

    if 'ä½œæ¥­æ™‚é–“' in df.columns:
        df['ä½œæ¥­æ™‚é–“'] = pd.to_numeric(df['ä½œæ¥­æ™‚é–“'], errors='coerce')

    if 'éƒ¨å“æ•°é‡' in df.columns:
        df['éƒ¨å“æ•°é‡'] = pd.to_numeric(df['éƒ¨å“æ•°é‡'], errors='coerce')

    # Parse duration
    if 'çµŒéæ—¥æ•°' in df.columns:
        df['çµŒéæ—¥æ•°_æ—¥'] = df['çµŒéæ—¥æ•°'].apply(parse_duration_to_days)

    # Create base date for time analysis (å£²ä¸Šæ—¥ > ä½œæ¥­çµ‚äº†æ—¥ > ä½œæ¥­é–‹å§‹æ—¥)
    base_date_columns = ['å£²ä¸Šæ—¥', 'ä½œæ¥­çµ‚äº†æ—¥', 'ä½œæ¥­é–‹å§‹æ—¥']
    df['åŸºæº–æ—¥'] = pd.NaT

    for col in base_date_columns:
        if col in df.columns:
            if df['åŸºæº–æ—¥'].isna().all():
                df['åŸºæº–æ—¥'] = df[col]
            else:
                df['åŸºæº–æ—¥'] = df['åŸºæº–æ—¥'].fillna(df[col])

    # Create time-based columns
    if 'åŸºæº–æ—¥' in df.columns:
        df['å¹´'] = df['åŸºæº–æ—¥'].dt.year
        df['æœˆ'] = df['åŸºæº–æ—¥'].dt.month
        df['é€±'] = df['åŸºæº–æ—¥'].dt.isocalendar().week

    # Create hour bins
    if 'ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿_HR' in df.columns:
        df['ã‚¢ãƒ¯å¸¯'] = df['ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿_HR'].apply(create_hour_bins)

    # Create malfunction flag
    malfunction_columns = ['ã‚«ãƒ†ã‚´ãƒª', 'ä¸å…·åˆå†…å®¹', 'æ•…éšœçŠ¶æ³']
    df['ä¸å…·åˆãƒ•ãƒ©ã‚°'] = 0
    for col in malfunction_columns:
        if col in df.columns:
            mask = df[col].notna() & (df[col] != '') & (~df[col].str.contains('ç‚¹æ¤œ|æ¤œæŸ»', na=False))
            df.loc[mask, 'ä¸å…·åˆãƒ•ãƒ©ã‚°'] = 1

    # Extract bulletin numbers
    bulletin_columns = ['ä½œæ¥­å†…å®¹', 'ä½œæ¥­æ˜ç´°å‚™è€ƒ']
    df['ãƒ–ãƒ«ãƒãƒ³NO'] = np.nan
    for col in bulletin_columns:
        if col in df.columns:
            bulletin_nums = df[col].apply(extract_bulletin_number)
            df['ãƒ–ãƒ«ãƒãƒ³NO'] = df['ãƒ–ãƒ«ãƒãƒ³NO'].fillna(bulletin_nums)

    return df

def split_fact_tables(df: pd.DataFrame) -> tuple:
    """Split into case-level and parts-level fact tables"""
    if df.empty:
        return pd.DataFrame(), pd.DataFrame()

    # Case-level facts (unique by æ•´å‚™æ¡ˆä»¶NO)
    case_columns = [col for col in df.columns if col not in ['éƒ¨å“å“ç•ª', 'éƒ¨å“å“å', 'éƒ¨å“æ•°é‡']]
    if 'æ•´å‚™æ¡ˆä»¶NO' in df.columns:
        df_cases = df[case_columns].drop_duplicates(subset=['æ•´å‚™æ¡ˆä»¶NO'], keep='last')
    else:
        df_cases = df[case_columns].drop_duplicates()

    # Parts-level facts (all rows)
    df_parts = df.copy()

    return df_cases, df_parts

def apply_filters(df: pd.DataFrame, filters: Dict[str, Any]) -> pd.DataFrame:
    """Apply filters based on sidebar state"""
    if df.empty:
        return df

    filtered_df = df.copy()

    # Date range filter
    if filters.get('date_range') and 'åŸºæº–æ—¥' in filtered_df.columns:
        start_date, end_date = filters['date_range']
        if start_date:
            filtered_df = filtered_df[filtered_df['åŸºæº–æ—¥'] >= pd.to_datetime(start_date)]
        if end_date:
            filtered_df = filtered_df[filtered_df['åŸºæº–æ—¥'] <= pd.to_datetime(end_date)]

    # Categorical filters
    categorical_filters = ['æ©Ÿç¨®å', 'ã‚·ãƒªãƒ¼ã‚º', 'ç¨®åˆ¥', 'éƒ¨é–€å', 'æ•´å‚™åŒºåˆ†']
    for filter_name in categorical_filters:
        if filters.get(filter_name) and filter_name in filtered_df.columns:
            filtered_df = filtered_df[filtered_df[filter_name].isin(filters[filter_name])]

    # Hour meter filter
    if filters.get('hour_bins') and 'ã‚¢ãƒ¯å¸¯' in filtered_df.columns:
        filtered_df = filtered_df[filtered_df['ã‚¢ãƒ¯å¸¯'].isin(filters['hour_bins'])]

    # Exclude inspections toggle
    if filters.get('exclude_inspections', False):
        inspection_keywords = ['ç‚¹æ¤œ', 'æ¤œæŸ»', 'ç‰¹è‡ªæ¤œ', 'å®šæœŸç‚¹æ¤œ']
        if 'æ•´å‚™åŒºåˆ†' in filtered_df.columns:
            mask = ~filtered_df['æ•´å‚™åŒºåˆ†'].str.contains('|'.join(inspection_keywords), na=False)
            filtered_df = filtered_df[mask]

    # Apply sampling for performance
    if filters.get('use_sampling', False) and filters.get('sample_size'):
        sample_size = filters['sample_size']
        if len(filtered_df) > sample_size:
            filtered_df = filtered_df.sample(n=sample_size, random_state=42)

    return filtered_df

# ---------- Visualization Functions ----------

def render_dashboard(df_cases: pd.DataFrame, df_parts: pd.DataFrame, filters: Dict[str, Any]):
    """Render the main dashboard with required visualizations"""

    st.header("ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

    # Choose data based on aggregation unit
    df_active = df_parts if filters.get('aggregation_unit') == 'parts' else df_cases

    if df_active.empty:
        st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # 1. Machine type malfunction trends
    st.subheader("1. æ©Ÿç¨®åã”ã¨ã®ä¸å…·åˆå‚¾å‘")

    if 'æ©Ÿç¨®å' in df_active.columns and 'ä¸å…·åˆãƒ•ãƒ©ã‚°' in df_active.columns:
        malfunction_data = df_active[df_active['ä¸å…·åˆãƒ•ãƒ©ã‚°'] == 1]
        machine_counts = malfunction_data['æ©Ÿç¨®å'].value_counts().head(20)

        if not machine_counts.empty:
            fig = px.bar(
                x=machine_counts.values,
                y=machine_counts.index,
                orientation='h',
                title="æ©Ÿç¨®ååˆ¥ä¸å…·åˆä»¶æ•° (ä¸Šä½20)",
                labels={'x': 'ä»¶æ•°', 'y': 'æ©Ÿç¨®å'}
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)

            # Show detailed table
            with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
                total_counts = df_active['æ©Ÿç¨®å'].value_counts()
                detail_df = pd.DataFrame({
                    'æ©Ÿç¨®å': machine_counts.index,
                    'ä¸å…·åˆä»¶æ•°': machine_counts.values,
                    'ç·ä»¶æ•°': [total_counts.get(machine, 0) for machine in machine_counts.index],
                })
                detail_df['ä¸å…·åˆç‡(%)'] = (detail_df['ä¸å…·åˆä»¶æ•°'] / detail_df['ç·ä»¶æ•°'] * 100).round(2)
                st.dataframe(detail_df)

    # 2. Annual malfunction trends
    st.subheader("2. å¹´é–“ã®ä¸å…·åˆå‚¾å‘")

    if 'åŸºæº–æ—¥' in df_active.columns and 'ä¸å…·åˆãƒ•ãƒ©ã‚°' in df_active.columns:
        malfunction_data = df_active[df_active['ä¸å…·åˆãƒ•ãƒ©ã‚°'] == 1]
        if not malfunction_data.empty:
            monthly_data = malfunction_data.set_index('åŸºæº–æ—¥').resample('M').size()

            fig = px.line(
                x=monthly_data.index,
                y=monthly_data.values,
                title="æœˆåˆ¥ä¸å…·åˆä»¶æ•°æ¨ç§»",
                labels={'x': 'æœˆ', 'y': 'ä¸å…·åˆä»¶æ•°'}
            )
            st.plotly_chart(fig, use_container_width=True)

    # 3. Hour meter vs malfunction trends
    st.subheader("3. ã‚¢ãƒ¯ãƒ¼ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ä¸å…·åˆå‚¾å‘")

    if 'ã‚¢ãƒ¯å¸¯' in df_active.columns and 'ä¸å…·åˆãƒ•ãƒ©ã‚°' in df_active.columns:
        hour_malfunction = df_active[df_active['ä¸å…·åˆãƒ•ãƒ©ã‚°'] == 1]['ã‚¢ãƒ¯å¸¯'].value_counts()

        if not hour_malfunction.empty:
            fig = px.bar(
                x=hour_malfunction.index,
                y=hour_malfunction.values,
                title="ã‚¢ãƒ¯å¸¯åˆ¥ä¸å…·åˆä»¶æ•°",
                labels={'x': 'ã‚¢ãƒ¯å¸¯', 'y': 'ä¸å…·åˆä»¶æ•°'}
            )
            st.plotly_chart(fig, use_container_width=True)

    # 4. Parts replacement ratio for malfunctions
    st.subheader("4. ä¸å…·åˆã«å¯¾ã™ã‚‹äº¤æ›éƒ¨å“æ¯”ç‡")

    if 'éƒ¨å“å“å' in df_parts.columns and 'ä¸å…·åˆãƒ•ãƒ©ã‚°' in df_parts.columns:
        parts_malfunction = df_parts[df_parts['ä¸å…·åˆãƒ•ãƒ©ã‚°'] == 1]
        parts_counts = parts_malfunction['éƒ¨å“å“å'].value_counts().head(10)

        if not parts_counts.empty:
            fig = px.pie(
                values=parts_counts.values,
                names=parts_counts.index,
                title="ä¸å…·åˆæ™‚ã®äº¤æ›éƒ¨å“æ¯”ç‡ (ä¸Šä½10)"
            )
            st.plotly_chart(fig, use_container_width=True)

def render_additional_kpis(df_cases: pd.DataFrame, df_parts: pd.DataFrame):
    """Render additional KPI visualizations"""

    st.header("ğŸ“ˆ è¿½åŠ KPI")

    col1, col2 = st.columns(2)

    with col1:
        # Bulletin rankings
        if 'ãƒ–ãƒ«ãƒãƒ³NO' in df_cases.columns:
            bulletin_counts = df_cases['ãƒ–ãƒ«ãƒãƒ³NO'].value_counts().head(10)
            if not bulletin_counts.empty:
                fig = px.bar(
                    x=bulletin_counts.values,
                    y=bulletin_counts.index,
                    orientation='h',
                    title="ãƒ–ãƒ«ãƒãƒ³å¯¾å¿œãƒ©ãƒ³ã‚­ãƒ³ã‚°"
                )
                st.plotly_chart(fig, use_container_width=True)


    with col2:
        # Work location analysis
        if 'æ•´å‚™å ´æ‰€' in df_cases.columns:
            location_counts = df_cases['æ•´å‚™å ´æ‰€'].value_counts()
            if not location_counts.empty:
                fig = px.bar(
                    x=location_counts.index,
                    y=location_counts.values,
                    title="æ•´å‚™å ´æ‰€åˆ¥ä»¶æ•°"
                )
                st.plotly_chart(fig, use_container_width=True)

        # Department MTTR analysis
        if 'éƒ¨é–€å' in df_cases.columns and 'ä½œæ¥­æ™‚é–“' in df_cases.columns:
            dept_mttr = df_cases.groupby('éƒ¨é–€å')['ä½œæ¥­æ™‚é–“'].median().sort_values(ascending=False)
            if not dept_mttr.empty:
                fig = px.bar(
                    x=dept_mttr.index,
                    y=dept_mttr.values,
                    title="éƒ¨é–€åˆ¥MTTR (ä¸­å¤®å€¤ãƒ»æ™‚é–“)"
                )
                st.plotly_chart(fig, use_container_width=True)

def render_table(df: pd.DataFrame):
    """Render detailed table view"""
    st.header("ğŸ“‹ è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«")

    if df.empty:
        st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # Search functionality
    search_term = st.text_input("ğŸ” æ¤œç´¢", help="ãƒ†ãƒ¼ãƒ–ãƒ«å†…å®¹ã‚’æ¤œç´¢")

    if search_term:
        # Search across all text columns
        text_columns = df.select_dtypes(include=['object']).columns
        mask = df[text_columns].astype(str).apply(
            lambda x: x.str.contains(search_term, case=False, na=False)
        ).any(axis=1)
        df_filtered = df[mask]
    else:
        df_filtered = df

    st.write(f"è¡¨ç¤ºä»¶æ•°: {len(df_filtered):,} / å…¨ä½“: {len(df):,}")

    # Display dataframe
    st.dataframe(df_filtered, use_container_width=True, height=600)

def render_quality_report(df: pd.DataFrame):
    """Render data quality report"""
    st.header("ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")

    if df.empty:
        st.warning("åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("æ¬ æç‡åˆ†æ")
        missing_rates = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)
        missing_df = pd.DataFrame({
            'åˆ—å': missing_rates.index,
            'æ¬ æç‡(%)': missing_rates.values
        })
        missing_df = missing_df[missing_df['æ¬ æç‡(%)'] > 0]

        if not missing_df.empty:
            fig = px.bar(
                missing_df.head(20),
                x='æ¬ æç‡(%)',
                y='åˆ—å',
                orientation='h',
                title="æ¬ æç‡ä¸Šä½20åˆ—"
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.success("æ¬ æãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")

    with col2:
        st.subheader("é‡è¤‡ãƒ‡ãƒ¼ã‚¿åˆ†æ")
        if 'æ•´å‚™æ¡ˆä»¶NO' in df.columns:
            duplicate_counts = df['æ•´å‚™æ¡ˆä»¶NO'].value_counts()
            duplicates = duplicate_counts[duplicate_counts > 1]

            if not duplicates.empty:
                st.warning(f"é‡è¤‡ã™ã‚‹æ•´å‚™æ¡ˆä»¶NO: {len(duplicates)}ä»¶")
                st.dataframe(duplicates.head(10).to_frame('ä»¶æ•°'))
            else:
                st.success("é‡è¤‡ã™ã‚‹æ•´å‚™æ¡ˆä»¶NOã¯ã‚ã‚Šã¾ã›ã‚“")

        # Value counts for key categorical fields
        st.subheader("ä¸»è¦é …ç›®ã®å€¤åˆ†å¸ƒ")
        key_fields = ['æ©Ÿç¨®å', 'ã‚·ãƒªãƒ¼ã‚º', 'æ•´å‚™åŒºåˆ†', 'ä¿è¨¼åŒºåˆ†']
        for field in key_fields:
            if field in df.columns:
                value_counts = df[field].value_counts().head(5)
                if not value_counts.empty:
                    st.write(f"**{field}** (ä¸Šä½5)")
                    st.dataframe(value_counts.to_frame('ä»¶æ•°'))

# ---------- Q&A Chat Functions ----------

def llm_to_json_spec(question: str, columns: List[str]) -> Dict[str, Any]:
    """Convert user question to structured query specification using OpenAI"""

    # Get OpenAI API key
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable is not set")

    client = OpenAI(api_key=api_key)

    system_prompt = f"""ã‚ãªãŸã¯å»ºæ©Ÿã®ä¿®ç†ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªã®è³ªå•ã‚’ã€pandasã§å®Ÿè¡Œå¯èƒ½ãªé›†è¨ˆä»•æ§˜ã®JSONã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿åˆ—åã¯æ—¥æœ¬èªã§ã™ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—å: {', '.join(columns)}

å‡ºåŠ›JSONã‚¹ã‚­ãƒ¼ãƒï¼š
{{
  "time_range": {{"from": "YYYY-MM-DD|null", "to": "YYYY-MM-DD|null"}},
  "filters": [{{"column": "åˆ—å", "op": "==|!=|in|not in|contains|regex|fuzzy_search", "value": "any"}}, ...],
  "groupby": ["åˆ—å", ...],
  "metrics": [{{"name":"ä»¶æ•°","expr":"count"}}, {{"name":"æ•°é‡åˆè¨ˆ","expr":"sum:éƒ¨å“æ•°é‡"}}, ...],
  "topn": 10,
  "sort": {{"by":"ä»¶æ•°","asc":false}},
  "free_text_search": {{"enabled": true|false, "keywords": ["keyword1", "keyword2"], "target_columns": ["æ•…éšœçŠ¶æ³", "ä¸å…·åˆå†…å®¹", "ä½œæ¥­å†…å®¹", "ä½œæ¥­æ˜ç´°å‚™è€ƒ", "éƒ¨å“å“å"]}}
}}

**é‡è¦**: metrics[].name ã¯æœ€çµ‚ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ—åã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
- expr:"count" ã®å ´åˆ: groupbyã®ã‚µã‚¤ã‚ºï¼ˆå…¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼‰ã‚’é›†è¨ˆã—ã€åˆ—åã¯å¿…ãšnameï¼ˆä¾‹ï¼š"name": "ä»¶æ•°"ï¼‰
- expr:"sum:<åˆ—å>" ã®å ´åˆ: åˆè¨ˆçµæœã®åˆ—åã¯å¿…ãšnameï¼ˆä¾‹ï¼š"name": "æ•°é‡åˆè¨ˆ"ï¼‰
- äººé–“ãŒç†è§£ã—ã‚„ã™ã„åˆ—åã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆã€Œä»¶æ•°ã€ã€Œåˆè¨ˆã€ã€Œå¹³å‡å€¤ã€ãªã©ï¼‰

ç‰¹åˆ¥ãªæ¤œç´¢æ©Ÿèƒ½:
- fuzzy_search: ã‚ã„ã¾ã„æ¤œç´¢ï¼ˆé¡ä¼¼ã—ãŸæ–‡å­—åˆ—ã‚‚æ¤œç´¢ï¼‰
- free_text_search: ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆè¤‡æ•°ã®åˆ—ã‚’æ¨ªæ–­ã—ã¦æ¤œç´¢ï¼‰

æ•…éšœç¾è±¡ã‚„ä½œæ¥­å†…å®¹ã«é–¢ã™ã‚‹è³ªå•ã®å ´åˆã¯ã€free_text_search ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚
è³ªå•ã«æ›–æ˜§ã•ãŒã‚ã‚‹å ´åˆã¯ã€å¸¸è­˜çš„ã«è£œå®Œã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œ1å¹´ã€ã¯ç›´è¿‘365æ—¥ï¼‰ã€‚
JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            temperature=0.1,
            max_tokens=1000
        )

        json_text = response.choices[0].message.content.strip()
        # Remove code block markers if present
        json_text = re.sub(r'^```json\s*|\s*```$', '', json_text, flags=re.MULTILINE)

        return json.loads(json_text)

    except Exception as e:
        error_msg = f"OpenAI API ã‚¨ãƒ©ãƒ¼: {str(e)}"

        # Add helpful information for common errors
        if "API key" in str(e) or "authentication" in str(e).lower():
            error_msg += "\n\nğŸ’¡ API ã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
        elif "rate limit" in str(e).lower() or "quota" in str(e).lower():
            error_msg += "\n\nğŸ’¡ APIä½¿ç”¨é‡åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„"
        elif "timeout" in str(e).lower():
            error_msg += "\n\nğŸ’¡ æ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å†è©¦è¡Œã—ã¦ãã ã•ã„"

        st.error(error_msg)
        return {}

def execute_spec(df_cases: pd.DataFrame, df_parts: pd.DataFrame, spec: Dict[str, Any]) -> pd.DataFrame:
    """Execute query specification safely on dataframes"""

    try:
        # Choose appropriate dataframe
        if any('éƒ¨å“' in str(metric.get('expr', '')) for metric in spec.get('metrics', [])):
            df = df_parts.copy()
        else:
            df = df_cases.copy()

        if df.empty:
            return pd.DataFrame()

        # Apply time range filter
        time_range = spec.get('time_range', {})
        if time_range.get('from') and 'åŸºæº–æ—¥' in df.columns:
            df = df[df['åŸºæº–æ—¥'] >= pd.to_datetime(time_range['from'])]
        if time_range.get('to') and 'åŸºæº–æ—¥' in df.columns:
            df = df[df['åŸºæº–æ—¥'] <= pd.to_datetime(time_range['to'])]

        # Apply filters
        for filter_item in spec.get('filters', []):
            column = filter_item.get('column')
            op = filter_item.get('op')
            value = filter_item.get('value')

            if column not in df.columns:
                continue

            if op == '==':
                df = df[df[column] == value]
            elif op == '!=':
                df = df[df[column] != value]
            elif op == 'in':
                df = df[df[column].isin(value if isinstance(value, list) else [value])]
            elif op == 'not in':
                df = df[~df[column].isin(value if isinstance(value, list) else [value])]
            elif op == 'contains':
                df = df[df[column].astype(str).str.contains(str(value), na=False, case=False)]
            elif op == 'regex':
                df = df[df[column].astype(str).str.contains(str(value), na=False, case=False, regex=True)]
            elif op == 'fuzzy_search':
                # Fuzzy search using rapidfuzz
                mask = df[column].astype(str).apply(
                    lambda x: fuzz.partial_ratio(str(value).lower(), str(x).lower()) > 70
                )
                df = df[mask]

        # Apply free text search if specified
        free_text_search = spec.get('free_text_search', {})
        if free_text_search.get('enabled', False):
            keywords = free_text_search.get('keywords', [])
            target_columns = free_text_search.get('target_columns', [])

            if keywords and target_columns:
                # Create search mask across multiple columns
                search_mask = pd.Series([False] * len(df), index=df.index)

                for keyword in keywords:
                    for col in target_columns:
                        if col in df.columns:
                            col_mask = df[col].astype(str).str.contains(
                                keyword, na=False, case=False, regex=False
                            )
                            search_mask = search_mask | col_mask

                df = df[search_mask]

        # Get groupby columns
        groupby_cols = [col for col in spec.get('groupby', []) if col in df.columns]

        # Process metrics safely - create DataFrame for each metric and merge
        metric_frames = []
        for metric in spec.get('metrics', []):
            metric_name = metric.get('name')
            expr = metric.get('expr', 'count')

            if not metric_name:
                continue

            if groupby_cols:
                # With groupby
                grouped = df.groupby(groupby_cols, dropna=False)

                if expr == 'count':
                    # Use size() to get total record count
                    metric_result = grouped.size().rename(metric_name).reset_index()
                elif expr.startswith('sum:'):
                    col_name = expr.split(':', 1)[1]
                    if col_name not in df.columns:
                        df[col_name] = 0  # Create missing column with zeros
                    # Convert to numeric and handle errors
                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0)
                    metric_result = grouped[col_name].sum().rename(metric_name).reset_index()
                elif expr.startswith('mean:'):
                    col_name = expr.split(':', 1)[1]
                    if col_name not in df.columns:
                        df[col_name] = 0
                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0)
                    metric_result = grouped[col_name].mean().rename(metric_name).reset_index()
                else:
                    raise ValueError(f"Unsupported expression: {expr}")

            else:
                # No groupby - global aggregation
                if expr == 'count':
                    metric_result = pd.DataFrame({metric_name: [len(df)]})
                elif expr.startswith('sum:'):
                    col_name = expr.split(':', 1)[1]
                    if col_name not in df.columns:
                        df[col_name] = 0
                    col_sum = pd.to_numeric(df[col_name], errors='coerce').fillna(0).sum()
                    metric_result = pd.DataFrame({metric_name: [col_sum]})
                elif expr.startswith('mean:'):
                    col_name = expr.split(':', 1)[1]
                    if col_name not in df.columns:
                        df[col_name] = 0
                    col_mean = pd.to_numeric(df[col_name], errors='coerce').fillna(0).mean()
                    metric_result = pd.DataFrame({metric_name: [col_mean]})
                else:
                    raise ValueError(f"Unsupported expression: {expr}")

            metric_frames.append(metric_result)

        # Combine all metrics
        if not metric_frames:
            # Default to count if no metrics specified
            if groupby_cols:
                result_df = df.groupby(groupby_cols, dropna=False).size().rename('ä»¶æ•°').reset_index()
            else:
                result_df = pd.DataFrame({'ä»¶æ•°': [len(df)]})
        else:
            result_df = metric_frames[0]
            for metric_frame in metric_frames[1:]:
                if groupby_cols:
                    # Merge on groupby columns
                    result_df = result_df.merge(metric_frame, on=groupby_cols, how='outer')
                else:
                    # Concatenate columns for global metrics
                    result_df = pd.concat([result_df, metric_frame], axis=1)

        # Safe sorting
        sort_spec = spec.get('sort', {})
        sort_by = sort_spec.get('by', 'ä»¶æ•°')
        sort_asc = sort_spec.get('asc', False)

        # Check if sort column exists, fallback to first metric column
        if sort_by not in result_df.columns:
            metric_names = [m.get('name') for m in spec.get('metrics', []) if m.get('name') in result_df.columns]
            if metric_names:
                sort_by = metric_names[0]
            elif 'ä»¶æ•°' in result_df.columns:
                sort_by = 'ä»¶æ•°'
            else:
                sort_by = None

        if sort_by and sort_by in result_df.columns:
            # Convert to numeric for proper sorting
            result_df[sort_by] = pd.to_numeric(result_df[sort_by], errors='ignore')
            result_df = result_df.sort_values(by=sort_by, ascending=sort_asc, kind='mergesort')

        # Apply topn limit
        topn = spec.get('topn')
        if isinstance(topn, int) and topn > 0:
            result_df = result_df.head(topn)

        # Clean up and return
        return result_df.reset_index(drop=True)

    except Exception as e:
        error_msg = f"ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"

        # Add helpful debugging information
        if "Column(s)" in str(e) and "do not exist" in str(e):
            error_msg += "\n\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:\n"
            error_msg += "â€¢ ã‚ˆã‚Šåºƒã„æ¡ä»¶ã§æ¤œç´¢ã™ã‚‹\n"
            error_msg += "â€¢ æœŸé–“ã‚’å¤‰æ›´ã™ã‚‹\n"
            error_msg += "â€¢ ç•°ãªã‚‹é …ç›®ã§é›†è¨ˆã™ã‚‹"

        st.error(error_msg)
        return pd.DataFrame()

def generate_natural_response(question: str, result_df: pd.DataFrame, spec: Dict[str, Any]) -> str:
    """Generate natural language response for query results"""

    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        return "åˆ†æçµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"

    client = OpenAI(api_key=api_key)

    # Prepare context about the results
    result_summary = f"è³ªå•: {question}\n\n"

    if result_df.empty:
        result_summary += "è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    else:
        result_summary += f"æ¤œç´¢çµæœ: {len(result_df)}ä»¶\n"
        result_summary += result_df.to_string(index=False, max_rows=10)

        if len(result_df) > 10:
            result_summary += "\n...ï¼ˆä»–ã®çµæœã¯çœç•¥ï¼‰"

    system_prompt = """ã‚ãªãŸã¯å»ºæ©Ÿã®ä¿®ç†ãƒ‡ãƒ¼ã‚¿åˆ†æå°‚é–€ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
åˆ†æçµæœã‚’åŸºã«ã€è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
- æ•°å€¤ã¯å…·ä½“çš„ã«ç¤ºã™
- å‚¾å‘ã‚„ç‰¹å¾´ãŒã‚ã‚Œã°æŒ‡æ‘˜ã™ã‚‹
- å®Ÿç”¨çš„ãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹
- ç°¡æ½”ã§æ˜ç¢ºãªè¡¨ç¾ã‚’ä½¿ã†
- å¿…è¦ã«å¿œã˜ã¦æ”¹å–„ææ¡ˆã‚’å«ã‚ã‚‹

200æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": result_summary}
            ],
            temperature=0.3,
            max_tokens=300
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"åˆ†æçµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ï¼ˆè‡ªç„¶è¨€èªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}ï¼‰"

def qa_chat(df_cases: pd.DataFrame, df_parts: pd.DataFrame):
    """Render Q&A chat interface"""

    st.header("ğŸ’¬ QAãƒãƒ£ãƒƒãƒˆ")
    st.write("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚")

    # Show example questions
    st.info("ğŸ’¡ è³ªå•ä¾‹:\n" +
            "ãƒ»ã€Œã‚ªã‚¤ãƒ«æ¼ã‚Œã®ä»¶æ•°ã¯ï¼Ÿã€\n" +
            "ãƒ»ã€Œã‚¨ãƒ³ã‚¸ãƒ³é–¢é€£ã®æ•…éšœã‚’æ•™ãˆã¦ã€\n" +
            "ãƒ»ã€Œä»Šå¹´ã®ãƒˆãƒ©ãƒ–ãƒ«ä¸Šä½3ä½ã¯ï¼Ÿã€\n" +
            "ãƒ»ã€Œæ²¹åœ§ç³»çµ±ã®å•é¡ŒãŒã‚ã£ãŸæ©Ÿç¨®ã¯ï¼Ÿã€\n" +
            "ãƒ»ã€Œæœ€è¿‘ã®ãƒãƒ³ãƒ—äº¤æ›ã®å‚¾å‘ã¯ï¼Ÿã€")

    # Check for OpenAI API key
    if not os.environ.get('OPENAI_API_KEY'):
        st.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return

    # Initialize chat history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # Chat history management
    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        st.write(f"ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´: {len(st.session_state.chat_history)}ä»¶")

    with col2:
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", help="ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã™ã¹ã¦ã‚¯ãƒªã‚¢ã—ã¾ã™"):
            st.session_state.chat_history = []
            st.rerun()

    with col3:
        max_history = st.selectbox(
            "æœ€å¤§å±¥æ­´æ•°",
            options=[10, 20, 50, 100],
            index=1,  # Default to 20 (index 1)
            help="ä¿æŒã™ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æœ€å¤§æ•°"
        )

    # Limit chat history to prevent memory issues
    if len(st.session_state.chat_history) > max_history:
        st.session_state.chat_history = st.session_state.chat_history[-max_history:]

    # Display chat history
    for i, chat in enumerate(st.session_state.chat_history):
        with st.chat_message(chat["role"]):
            st.write(chat["content"])
            if "dataframe" in chat:
                # Limit dataframe display size for performance
                df_display = chat["dataframe"]
                if len(df_display) > 100:
                    st.write(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_display)}ä»¶ (ä¸Šä½100ä»¶ã‚’è¡¨ç¤º)")
                    df_display = df_display.head(100)
                st.dataframe(df_display, use_container_width=True, height=300)

    # Chat input
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        # Ensure chat history list exists and is writable
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        # Limit prompt length to prevent API issues
        if len(prompt) > 1000:
            st.warning("è³ªå•ãŒé•·ã™ãã¾ã™ã€‚1000æ–‡å­—ä»¥å†…ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            # Add user message to chat history
            try:
                st.session_state.chat_history.append({"role": "user", "content": prompt})

                with st.chat_message("user"):
                    st.write(prompt)

                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner("åˆ†æä¸­..."):
                        try:
                            # Get available columns
                            available_columns = list(set(df_cases.columns.tolist() + df_parts.columns.tolist()))

                            # Generate query specification
                            spec = llm_to_json_spec(prompt, available_columns)

                            if spec:
                                # Debug information (show in expander)
                                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                                    st.json(spec)
                                    st.write(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {len(available_columns)}å€‹")
                                    st.write(f"å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {'éƒ¨å“ãƒ‡ãƒ¼ã‚¿' if any('éƒ¨å“' in str(metric.get('expr', '')) for metric in spec.get('metrics', [])) else 'æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿'}")

                                # Execute query
                                result_df = execute_spec(df_cases, df_parts, spec)

                                if not result_df.empty:
                                    # Generate natural language response
                                    response_text = generate_natural_response(prompt, result_df, spec)

                                    st.write(response_text)
                                    st.dataframe(result_df, use_container_width=True)

                                    # Add to chat history
                                    st.session_state.chat_history.append({
                                        "role": "assistant",
                                        "content": response_text,
                                        "dataframe": result_df
                                    })
                                else:
                                    # More specific error message with debugging info
                                    error_msg = "è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"

                                    # Show what was searched
                                    if spec.get('filters'):
                                        error_msg += "é©ç”¨ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿:\n"
                                        for f in spec.get('filters', []):
                                            error_msg += f"â€¢ {f.get('column')} {f.get('op')} {f.get('value')}\n"

                                    if spec.get('time_range', {}).get('from') or spec.get('time_range', {}).get('to'):
                                        time_from = spec.get('time_range', {}).get('from', 'å§‹æœŸãªã—')
                                        time_to = spec.get('time_range', {}).get('to', 'çµ‚æœŸãªã—')
                                        error_msg += f"æœŸé–“: {time_from} - {time_to}\n"

                                    # Show expected columns vs available columns
                                    expected_metrics = [m.get('name', 'unknown') for m in spec.get('metrics', [])]
                                    if expected_metrics:
                                        error_msg += f"æœŸå¾…ã•ã‚Œã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {', '.join(expected_metrics)}\n"

                                    error_msg += "\nğŸ’¡ ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„:\n"
                                    error_msg += "â€¢ ã‚ˆã‚Šåºƒã„æ¡ä»¶ã§æ¤œç´¢ã™ã‚‹\n"
                                    error_msg += "â€¢ æœŸé–“ã‚’å¤‰æ›´ã™ã‚‹\n"
                                    error_msg += "â€¢ ç•°ãªã‚‹é …ç›®ã§é›†è¨ˆã™ã‚‹\n"
                                    error_msg += "â€¢ ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹"

                                    st.write(error_msg)
                                    st.session_state.chat_history.append({"role": "assistant", "content": error_msg})
                            else:
                                error_msg = "è³ªå•ã‚’ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n\nğŸ’¡ ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„:\n"
                                error_msg += "â€¢ ã€Œå…¨ä½“ã®ä»¶æ•°ã¯ï¼Ÿã€\n"
                                error_msg += "â€¢ ã€Œæ©Ÿç¨®åˆ¥ã®ä»¶æ•°ã‚’æ•™ãˆã¦ã€\n"
                                error_msg += "â€¢ ã€Œä»Šæœˆã®ä¸å…·åˆä»¶æ•°ã¯ï¼Ÿã€\n"
                                error_msg += "â€¢ ã€Œç—‡çŠ¶åˆ¥ã®ä»¶æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€"
                                st.write(error_msg)
                                st.session_state.chat_history.append({"role": "assistant", "content": error_msg})

                        except Exception as e:
                            error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                            st.error(error_msg)
                            st.session_state.chat_history.append({"role": "assistant", "content": error_msg})

            except Exception as e:
                # Handle outer exception for chat history operations
                st.error(f"ãƒãƒ£ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                if "chat_history" in st.session_state:
                    st.session_state.chat_history.append({
                        "role": "assistant",
                        "content": f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                    })

# ---------- Main Application ----------

def main():
    """Main application function"""

    st.set_page_config(
        page_title=APP_TITLE,
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title(APP_TITLE)

    # Sidebar
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # File selection
    selected_files = st.sidebar.multiselect(
        "ğŸ“ èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«",
        DEFAULT_FILES,
        default=[DEFAULT_FILES[0]],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        help="åˆ†æå¯¾è±¡ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è€ƒæ…®ã—ã¦ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿é¸æŠï¼‰"
    )

    if not selected_files:
        st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return

    # Load data
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df_raw = load_excel(selected_files)

        if df_raw.empty:
            st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        df_normalized = normalize_df(df_raw)
        df_cases, df_parts = split_fact_tables(df_normalized)

    st.sidebar.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df_raw):,}è¡Œ")

    # Filters
    st.sidebar.subheader("ğŸ” ãƒ•ã‚£ãƒ«ã‚¿")

    filters = {}

    # Date range filter
    if 'åŸºæº–æ—¥' in df_normalized.columns:
        min_date = df_normalized['åŸºæº–æ—¥'].min()
        max_date = df_normalized['åŸºæº–æ—¥'].max()

        if pd.notna(min_date) and pd.notna(max_date):
            date_range = st.sidebar.date_input(
                "ğŸ“… æœŸé–“",
                value=(min_date.date(), max_date.date()),
                min_value=min_date.date(),
                max_value=max_date.date(),
                help="åˆ†æå¯¾è±¡æœŸé–“ã‚’é¸æŠ"
            )

            if len(date_range) == 2:
                filters['date_range'] = date_range

    # Categorical filters
    categorical_fields = {
        'æ©Ÿç¨®å': 'ğŸšœ',
        'ã‚·ãƒªãƒ¼ã‚º': 'ğŸ“‹',
        'ç¨®åˆ¥': 'ğŸ·ï¸',
        'éƒ¨é–€å': 'ğŸ¢',
        'æ•´å‚™åŒºåˆ†': 'ğŸ”§'
    }

    for field, icon in categorical_fields.items():
        if field in df_normalized.columns:
            unique_values = df_normalized[field].dropna().unique()
            if len(unique_values) > 0:
                selected_values = st.sidebar.multiselect(
                    f"{icon} {field}",
                    unique_values,
                    help=f"{field}ã§ãƒ•ã‚£ãƒ«ã‚¿"
                )
                if selected_values:
                    filters[field] = selected_values

    # Hour meter filter
    if 'ã‚¢ãƒ¯å¸¯' in df_normalized.columns:
        hour_bins = df_normalized['ã‚¢ãƒ¯å¸¯'].unique()
        selected_hour_bins = st.sidebar.multiselect(
            "â±ï¸ ã‚¢ãƒ¯å¸¯",
            hour_bins,
            help="ã‚¢ãƒ¯ãƒ¡ãƒ¼ã‚¿å¸¯åŸŸã§ãƒ•ã‚£ãƒ«ã‚¿"
        )
        if selected_hour_bins:
            filters['hour_bins'] = selected_hour_bins

    # Options
    st.sidebar.subheader("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    # Data sampling option for performance
    use_sampling = st.sidebar.checkbox(
        "é«˜é€Ÿè¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰",
        value=True,
        help="å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦é«˜é€Ÿè¡¨ç¤ºã—ã¾ã™"
    )

    if use_sampling:
        sample_size = st.sidebar.slider(
            "ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º",
            min_value=1000,
            max_value=50000,
            value=10000,
            step=1000,
            help="è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ï¼ˆå¤šã„ã»ã©æ­£ç¢ºã€å°‘ãªã„ã»ã©é«˜é€Ÿï¼‰"
        )
    else:
        sample_size = None

    filters['use_sampling'] = use_sampling
    filters['sample_size'] = sample_size

    filters['exclude_inspections'] = st.sidebar.checkbox(
        "ç‚¹æ¤œæ•´å‚™ã‚’é™¤å¤–",
        help="ç‚¹æ¤œãƒ»æ¤œæŸ»é …ç›®ã‚’é›†è¨ˆã‹ã‚‰é™¤å¤–"
    )

    filters['aggregation_unit'] = st.sidebar.radio(
        "é›†è¨ˆå˜ä½",
        ["cases", "parts"],
        format_func=lambda x: "æ¡ˆä»¶å˜ä½" if x == "cases" else "éƒ¨å“æ˜ç´°å˜ä½",
        help="ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆç²’åº¦ã‚’é¸æŠ"
    )

    # Apply filters
    df_cases_filtered = apply_filters(df_cases, filters)
    df_parts_filtered = apply_filters(df_parts, filters)

    st.sidebar.write(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œä»¶æ•°: {len(df_cases_filtered):,}æ¡ˆä»¶")

    # Show sampling info
    if filters.get('use_sampling', False) and filters.get('sample_size'):
        original_count = len(df_cases)
        if len(df_cases_filtered) < original_count:
            st.sidebar.info(f"âš¡ é«˜é€Ÿè¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰: {filters['sample_size']:,}è¡Œã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¡¨ç¤ºä¸­")

    # Download button
    if not df_cases_filtered.empty:
        csv_data = df_cases_filtered.to_csv(index=False, encoding='utf-8-sig')
        st.sidebar.download_button(
            "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            csv_data,
            "kubota_filtered_data.csv",
            "text/csv",
            help="ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
        )

    # Main content tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "ğŸ“‹ è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«",
        "ğŸ’¬ QAãƒãƒ£ãƒƒãƒˆ",
        "ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ª"
    ])

    with tab1:
        render_dashboard(df_cases_filtered, df_parts_filtered, filters)
        render_additional_kpis(df_cases_filtered, df_parts_filtered)

    with tab2:
        active_df = df_parts_filtered if filters.get('aggregation_unit') == 'parts' else df_cases_filtered
        render_table(active_df)

    with tab3:
        qa_chat(df_cases_filtered, df_parts_filtered)

    with tab4:
        render_quality_report(df_normalized)

if __name__ == "__main__":
    main()